from pyspark import SparkConf,SparkContext
from pyspark.streaming import StreamingContext
from pyspark.sql import Row,SQLContext,SparkSession
from pyspark.sql.types import *
import sys
import requests

def FirstUpperCase(input_string) :
    newString = "%s%s" % (input_string[0].upper(), input_string[1:].lower())
    return newString

# def aggregate_tags_count(new_values, total_sum):
# 	return sum(new_values) + (total_sum or 0)
# ICI  ON DEVRAIT POUVOIR IDENTIFIER LES NOUVEAUX HASHTAGS
conf = SparkConf().setAppName("Lambda_Batch_Processor").setMaster("local")
sc = SparkContext(conf)
spark = SparkSession.builder.getOrCreate()

# L'idée c'est de créer une df avec (id,text,hashtags) et de récupérer ensuite la liste des hashtags avec un select
tweetdb = spark.sql("CREATE DATABASE IF NOT EXISTS tweetdb")
spark.catalog.setCurrentDatabase('tweetdb')
schema = StructType()\
        .add("id",IntType(),True)\
        .add("text",StringType(),True)\
        .add("hashtags",StringType(),True)

# # create the Streaming Context from the above spark context with interval size 2 seconds
# ssc = StreamingContext(sc, 2)
# # setting a checkpoint to allow RDD recovery
# ssc.checkpoint("checkpoint_TwitterApp")
# # read data from port 9009
# dataStream = ssc.socketTextStream("localhost",9009)
# split each tweet into words
words = dataStream.flatMap(lambda line: line.split(" "))
# filter the words to get only hashtags, then map each hashtag to be a pair of (hashtag,1)
hashtags = words.filter(lambda w: '#' in w).map(lambda x: (x, 1))
# adding the count of each hashtag to its last count
tags_totals = hashtags.updateStateByKey(aggregate_tags_count)








## Subscribe to 1 topic defaults to the earliest and latest offsets
# df = spark \
#   .read \ #readStream?
#   .format("kafka") \
#   .option("kafka.bootstrap.servers", "host1:port1,host2:port2") \
#   .option("subscribe", "topic1") \
#   .option("includeHeaders", "true") \
#   .load()
# df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")
# df.show()
# hashtags_df = df.select("hashtags")
# hashtags_df.write.parquet("hashtags.parquet")
